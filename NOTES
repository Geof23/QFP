This is my notes on QFP: how we're going to detect differences in FP results across processors, co-processors, fp hardware, compilers, and libraries:

To begin, it may be easier to target known differences in the components.

One problem with dot product 'perpendicularity' tests is that we:
* get a product of co-located items in vectors (i.e. a_0 * b_0 ( * ... * z_0))
* add results (r_0 + r_1 + ... + r_n)
The problem with addition with differences in magnitudes doesn't hold: i.e. for perpendicular vectors, the magnitudes of the summed products have to be identical (for D = 2) . . . could find more interesting examples with D > 2.  For my understanding, this is one of the biggest 'gotchas'.

So I've done some work with D = 2, or at least I've started a little framework for doing bitwise manipulations of fp numbers and comparing results . . .

But my point is that we may want to target known discrepancies and differences in the FP operation models.

Here is my brainstorm on differences that can be exploited for detecting anomalities or circumstances where fp results will not match:

******

from http://christian-seiler.de/projekte/fpmath/

* intermediate representations:
** x87 and powerPC use high precision intermediates
*** powerPC uses 64 bit precision for all fp representations
the short story, some platforms perform same ops on all operand types, others depend on precision of operands

* calling conventions can determine again a similar problem -- fp registers can be used to pass data directly from one function call to the next, keeping a-n intermediate result and not roudning / truncating into the destination memory type between operations

* lack of support for double precision intermediates

* has a great snippet of compiler reorderings


*** THE TEST SUITE ***

OK, so we have this collection of classes/structs so far:

* FPWrap -- handles automatic bit projection onto other types
* FPHelper -- does things like the actual bit projection to differing types, extraction of
  FP components (like the exponent bitfield), and other low-level bitwise operations on
  floating point
* Vector -- a simple multipurpose Vector templated class, providing things like multiply, L2 norm,
  dot product, etc
* Matrix -- a simple multipurpose templated class for matrices
* FPTests -- this is where the tests are written

** what we need **
* a script for running the tests
* command line option handling for the tests (i.e. a 'main' that can construct various
  tests (provided by FPTests) and interpret command line options like default fp precision,
  number of tests)


*** results ***

I'm not seeing a general difference between compilers.  Also, x86_64 machines don't generally support x87, so I think
those are beyond consideration (I mean, everyone doing serious HPC is using modern 64 bit machines)

So I'm experimenting with compiler options that will affect 64 bit floating point math (i.e. mmx / sse / avx instructions).

Here're the differences I'm coming up with:

*** -funsafe-math-optimizations

This shows a difference with g++ (5.x) on bihexal, but no difference with clang (3.6).

Here're the differences:

sawaya@bihexal:~/QFP/perpVects$ diff g++_bihexal_vanilla_out g++_bihexal_relaxed_out
33c33
< RotateAndUnrotate:    2.1684043449710088680149056017398834228515625e-19
---
> RotateAndUnrotate:    4.336808689942017736029811203479766845703125e-19
69c69
< RotateAndUnrotate:    2.1684043449710088680149056017398834228515625e-19
---
> RotateAndUnrotate:    4.336808689942017736029811203479766845703125e-19
105c105
< RotateAndUnrotate:    2.1684043449710088680149056017398834228515625e-19
---
> RotateAndUnrotate:    4.336808689942017736029811203479766845703125e-19
141c141
< RotateAndUnrotate:    2.1684043449710088680149056017398834228515625e-19
---
> RotateAndUnrotate:    4.336808689942017736029811203479766845703125e-19

I'm guessing that this is due to recognition of common constants (i.e. Pi)?
This test normally generates 0 error on float and double precision.

Also found a diff on intel and g++ vanilla on Kingspeak.  It is the OrthoPerturb test, which gets its error score from the L1Distance --
this isn't a very helpful score.  I suppose the score should be the distance of the perturbed' vector element from zero.  I wonder
if the diff will still show.  If not, we know how to create a diff anyway.


** 9-17-15 **
OK, so I want to do the following today:

* add a few more tests -- need to flex things like float unrepresentable values (0.1)



**** push for Ganesh's big NSF proposal (Zvonomir, Hari, Mary, John, Vivek) ****

Added Hari's Gram-Schmidt example

System notes:

* CloudLab A10
** mostly just seeing differences in optimization levels (all same size output save O2 and O3)
** The skewed matrix cross product rotation is the one comming up different

Need to see why this is (i.e. what are the underlying optimizations happening, and are they supersets of other FP opts)

* Kingspeak

